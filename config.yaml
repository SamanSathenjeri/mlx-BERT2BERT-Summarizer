model:
   num_embd: 256              # number of dimensions to represent the token
   num_layers: 4              # number of transformer blocks
   num_heads: 8               # Number of attention heads per block (head_dimension = num_embd//num_attention heads)
   hidden_embd: 1024          # Overall FeedForward dim size (4 * n_embd)
   vocab_size: 30522          # number of tokens in the vocabulary
   block_size: 64             # Context size

training:
   batch_size: 64             # Batches trained at the same time
   num_epochs: 10             # number of complete passes through the dataset
   learning_rate: 0.0003      # learning rate for training